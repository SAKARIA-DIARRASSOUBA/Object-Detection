{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Détection d'objet avec YOLO v3 sur les images personnalisées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO signifie \"You Only Look Once\" autrement dit qu'un seul réseau est appliqué une seule fois à l'image entière. YOLO est un algorithme de détection d'objets en utlisant des CNNs. Le récent YOLOv3 est plus puissant que le YOLO et le YOLOv2 de base et plus rapide que les algorithmes précédents comme R-CNN et plus précis aussi. D'abord YOLO divise l'image en régions, prédit les cadres de délimitation et les probabilités pour chacune de ces régions. YOLO prédit également la confiance pour chaque boîte englobante  qui contient réellement un objet et la probabilité de la classe de l'objet. Pour ce faire il scanne l'image (ou la trame) une seule fois pour faire une prédiction par rapport à un autre algorithme qui nécessite plusieurs analyses.\n",
    "\n",
    "Ensuite, les boîtes englobantes sont filtrées avec une technique appelée \"non-maximum suppression\" qui en exclut certaines si la confiance est faible ou s'il existe une autre boîte englobante pour cette région avec une confiance plus élevée.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importons les bibliothèques nécessaires\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargeons et lisaons l'image via la méthode  cv2.imread\n",
    "cv2.imread est une méthode charge une image à partir du fichier spécifié\n",
    "attention , si on ulise un window le chemin d'accès au fichier sera: 'images\\woman-working-in-the-office.jpg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_BGR = cv2.imread('images/yolo_image.jpeg', cv2.IMREAD_UNCHANGED)\n",
    "#image_BGR = cv2.imread('images/woman-working-in-the-office.jpg', cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "#Pour afficher l'image, utilisez le code suivant,\n",
    "cv2.imshow(\"image originale\", image_BGR)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redimensionner la taille de l'image\n",
    "Cette partie suivante premet de rediminsionner un image lorsqu'on suppose que la taille est très grande. Si vous décidez de redimensionner il faut tenir compte dans le reste du notebook. Cette partie est optionnelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dimensions :  (667, 1000, 3)\n",
      "redimensionnellement:  (400, 600, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Original Dimensions : ',image_BGR.shape)\n",
    " \n",
    "scale_percent = 60 # percent of original size\n",
    "width = int(image_BGR.shape[1] * scale_percent / 100)\n",
    "height = int(image_BGR.shape[0] * scale_percent / 100)\n",
    "dim = (width, height)\n",
    "  \n",
    "# redimensionner l'image pour reuire la taille de l'image\n",
    "resized = cv2.resize(image_BGR, dim, interpolation = cv2.INTER_AREA)\n",
    " \n",
    "print('redimensionnellement: ',resized.shape)\n",
    "\n",
    "#Pour afficher l'image, utilisez le code suivant,\n",
    " \n",
    "#cv2.imshow(\"image redimensionnee\", resized)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'image ci dessus est notre image originale à partir de laquelle on veut détecter autant d'objets que possible. Mais on ne peut pas donner cette image directement à l'algorithme, on doit donc effectuer une conversion à partir de cette image originale. C'est ce qu'on appelle la conversion d'objets blob, qui consiste essentiellement à extraire des fonctionnalités de l'image c'est une manière de prétraitement de l'image.\n",
    "\n",
    "L'entrée du réseau est ce que l'on appelle un objet blob . La fonction transforme l'image en un blob via **cv2.dnn.blobFromImage**.  Il a les paramètres suivants:\n",
    "- l' image à transformer\n",
    "- le facteur d' échelle (1/255 pour mettre à l'échelle les valeurs de pixel à [0..1])\n",
    "- la taille , ici une image carrée $416\\times416$\n",
    "- la valeur moyenne (par défaut = 0)\n",
    "- l'option swapBR = True (car OpenCV utilise BGR)\n",
    "\n",
    "la définition de l' indicateur True signifie que nous inverserons le bleu avec le rouge car OpenCV utilise BGR par contre on a des canaux dans l'image en RGB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = cv2.dnn.blobFromImage(image_BGR, 1 / 255.0, (416, 416),swapRB=True, crop=False)\n",
    "                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisons ce que ressemble les 3 objets blob différents en utilisant le code suivant. On n'observe pas beaucoup de différence mais c'est ce qu'on va donner à l'algorithme YOLO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in blob:\n",
    "    for n,img_blob in enumerate(b):\n",
    "        cv2.imshow(str(n),img_blob)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 416, 416)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charger l'architecture Darknet \n",
    "\n",
    "Chargeons l'algorithme yolo v3 en utilisant **cv2.dnn.readNetFromDarknet**, cela revient à charger des poids et un fichier cfg. Ensuite, nous chargerons les 80 labels des classes dans un tableau en utilisant le fichier coco.names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 9.06 µs\n",
      "label des classes de coco ['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
      "['yolo_82', 'yolo_94', 'yolo_106']\n",
      "Image height=667 and width=1000\n",
      "<class 'numpy.ndarray'>\n",
      "(80, 3)\n",
      "[164 137 107]\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "with open('yolo-coco-data/coco.names') as f:\n",
    "    labels = [line.strip() for line in f]\n",
    "#Chargement du détecteur d'objets YOLO v3 entraîné \n",
    "network = cv2.dnn.readNetFromDarknet('yolo-coco-data/yolov3.cfg',\n",
    "                                     'yolo-coco-data/yolov3.weights')\n",
    "\n",
    "print(\"label des classes de coco\",labels)\n",
    "# Obtenir la liste des noms de toutes les couches du réseau YOLO v3 \n",
    "layers_names_all = network.getLayerNames()\n",
    "# Obtenir uniquement les noms de couche de sortie dont on a besoin pour l'algo YOLO\n",
    "layers_names_output = [layers_names_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]\n",
    "#affichage des couches de sorties\n",
    "print(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']\n",
    "\n",
    "\n",
    "# la probabilité minimale pour éliminer les prédictions \n",
    "probability_minimum = 0.5\n",
    "#seuil lors de l'application de la technique non-maximum suppression\n",
    "threshold = 0.3\n",
    "\n",
    "\n",
    "#les couleurs pour représenter chaque objet détecté\n",
    "colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n",
    "h, w = image_BGR.shape[:2]\n",
    "print('Image height={0} and width={1}'.format(h, w))  \n",
    "\n",
    "\n",
    "# verification\n",
    "print(type(colours))  # <class 'numpy.ndarray'>\n",
    "print(colours.shape)  # (80, 3)\n",
    "print(colours[0])  # [172  10 127]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour Transmettre l'objet **blob** au réseau, on utilise la fonction **net.setInput (blob)** , puis aux couches de sorties. Ici, tous les objets détectés et à la sortie contiennent toutes les informations dont nous avons besoin pour extraire la position de l'objet comme les positions en haut, à gauche, à droite, en bas, le nom de la classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects Detection took 2.10118 seconds\n"
     ]
    }
   ],
   "source": [
    "network.setInput(blob)  \n",
    "start = time.time()\n",
    "output_from_network = network.forward(layers_names_output)\n",
    "end = time.time()\n",
    "\n",
    "# le temps nécessaire pour la Pass en avant\n",
    "print('Objects Detection took {:.5f} seconds'.format(end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédiction du modèle YOLO v3\n",
    "Ici, la confiance signifie à quel point l'algorithme est certain lorsqu'il prévoit un objet. Pour cela, on fera une boucle sur les **output_from_network**, pour avoir la confiance, class_numbers et bounding_boxes. Ensuite on récupère le class_numbers qui a la confiance la plus élevé parmi eux.\n",
    "\n",
    "Le seuil confiance minimum est à 0,5, alors toute confiance supérieur à 0,5 signifierait un objet détecté, ou soit aussi center_x, center_y, w (largeur), h (hauteur) de l'objet détecté. \n",
    "En outre , on dessinera un rectangle autour de l'objet détecté en utilisant center_x, center_y, w, h . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation des listes pour les boites de delimitation, \n",
    "#les cofiances obtenues et numero de classe\n",
    "\n",
    "bounding_boxes = []\n",
    "confidences = []\n",
    "class_numbers = []\n",
    "\n",
    "\n",
    "# Passage par toutes les couches de sortie après le forward pass\n",
    "for result in output_from_network:\n",
    "    # Passage par de toutes les détections de la couche de sortie courante\n",
    "    for detected_objects in result:\n",
    "        #  obtenir les 80 classes de proba pour les objets détectés \n",
    "        scores = detected_objects[5:]\n",
    "        # Obtenir l'indexe de la classe majoritaire\n",
    "        class_current = np.argmax(scores)\n",
    "        # Obtenir la valeur de proba  de cette classe\n",
    "        confidence_current = scores[class_current]\n",
    "\n",
    "        # Elimination des faibles prédictions \n",
    "        if confidence_current > probability_minimum:\n",
    "            # Le format de données YOLO conserve \n",
    "            #les coordonnées du centre du rectangle de délimitation ainsi \n",
    "            #que sa largeur et sa hauteur actuelles. C'est pourquoi on dois\n",
    "            #les multiplier par la largeur et la hauteur de l'image originale \n",
    "            #et obtenir ainsi les coordonnées du centre du rectangle \n",
    "            #de délimitation, sa largeur et sa hauteur pour l'image originale.\n",
    "            box_current = detected_objects[0:4] * np.array([w, h, w, h])\n",
    "\n",
    "           \n",
    "            x_center, y_center, box_width, box_height = box_current\n",
    "            #cv2.circle(image_BGR,(x_center, y_center),10,(0,255,0),2)\n",
    "            x_min = int(x_center - (box_width / 2))\n",
    "            y_min = int(y_center - (box_height / 2))\n",
    "\n",
    "            # ajouter aux listes\n",
    "            bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n",
    "            confidences.append(float(confidence_current))\n",
    "            class_numbers.append(class_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cv2.dnn.NMSBoxes(bounding_boxes, confidences,probability_minimum, threshold)\n",
    "                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object 1: person\n",
      "Object 2: person\n",
      "Object 3: laptop\n",
      "Object 4: chair\n",
      "\n",
      "Total objects been detected: 4\n",
      "Number of objects left after non-maximum suppression: 4\n"
     ]
    }
   ],
   "source": [
    "# un compteur pour les objets détecter\n",
    "counter = 1\n",
    "\n",
    "# Verifie s'il y a un objet détecté après le non-maximum suppression\n",
    "if len(results) > 0:\n",
    "    # Passage par les indexes des resultats\n",
    "    for i in results.flatten():\n",
    "        # le label de l'object détecté\n",
    "        print('Object {0}: {1}'.format(counter, labels[int(class_numbers[i])]))\n",
    "\n",
    "        # Incréménter le compteur\n",
    "        counter += 1\n",
    "\n",
    "        # Obtenir les coordonnées de la boite,\n",
    "     \n",
    "        x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
    "        box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n",
    "\n",
    "        # la couleur de la boite actuelle\n",
    "        #conversion d'un tableau numpy à une liste\n",
    "        colour_box_current = colours[class_numbers[i]].tolist()\n",
    "\n",
    "        \n",
    "        # Dessiner la boite de délimitation sur l'image originale\n",
    "        cv2.rectangle(image_BGR, (x_min, y_min),(x_min + box_width, y_min + box_height),\n",
    "                      colour_box_current, 2)\n",
    "\n",
    "        # Le texte pour le label et la confiance de la boite actuelle\n",
    "        text_box_current = '{}: {:.4f}'.format(labels[int(class_numbers[i])],\n",
    "                                               confidences[i])\n",
    "\n",
    "        # Mettre ce texte sur l'image originale\n",
    "        cv2.putText(image_BGR, text_box_current, (x_min, y_min - 5),\n",
    "                    cv2.FONT_HERSHEY_COMPLEX, 0.7, colour_box_current, 2)\n",
    "\n",
    "\n",
    "# Le nombre d'objets qui ont été détecté avant et après la technique\n",
    "print()\n",
    "print(\"Le total d'object detectés =\", len(bounding_boxes))\n",
    "print(\"Nombre d'objets restants après une non maximum suppression \", counter - 1)\n",
    "\n",
    "\n",
    "# affichage des objets détectés dans l'image\n",
    "cv2.namedWindow('Detections', cv2.WINDOW_NORMAL)\n",
    "cv2.imshow('Detections', image_BGR)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow('Detections')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-dcc379e8c4d3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-dcc379e8c4d3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    cv2 .__ version__\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "cv2 .__ version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
